# Данный файл - заметки выполненные в процессе прохождения курса по FastAPI и Celery
# https://testdriven.io/courses/fastapi-celery

# Установка Redis в докер:
docker run -p 6379:6379 --name some-redis -d redis

# Проверка работы Redis:
docker exec -it some-redis redis-cli ping

# Запуск приложения FastAPI:
uvicorn main:app --reload

# Запуск воркера Celery:
celery -A main.celery worker --loglevel=info
# Запуск воркера Celery (для Windows):
celery -A main.celery worker --loglevel=info --pool=solo

# Запуск первой задачи в Celery:
# > python
>>> from manage import run_demo_task
>>> task = run_demo_task()
# output:
# Running celery demo task "divide(1, 2)"
# 1 state=PENDING, result=None
# 2 state=PENDING, result=None
# 3 state=PENDING, result=None
# 4 state=PENDING, result=None
# 5 state=SUCCESS, result=0.5


# task = divide.delay(1, 0)
print(task.state, task.result)  # FAILURE ZeroDivisionError('division by zero')

# Запуск сервера мониторинга Flower:
celery -A main.celery flower --port=5555

# Получение статуса у ранее запущенной задачи по GUID
from celery.result import AsyncResult
task = AsyncResult('da04679c-fd30-4933-ae7b-3e1f5b941d90')
print(task.state, task.result)  # FAILURE ZeroDivisionError('division by zero')

# Инициализация alembic
alembic init alembic

# Создание пустой БД SQLite3
(env)$ python

>>> from main import app
>>> from project.database import Base, engine
>>> Base.metadata.create_all(bind=engine)
>>> exit()

# Запустить миграцию БД alembic
(env)$ alembic revision --autogenerate

# Создать новую миграцию alembic после добавления новой модели users
(env)$ alembic revision --autogenerate
# INFO  [alembic.autogenerate.compare] Detected added table 'users'

# Выполнить обновление БД до последней миграции
(env)$ alembic upgrade head
# Create users table


# Проверка работы Docker и Docker-compose
$ docker --version
Docker version 20.10.7, build f0df350

$ docker-compose --version
docker-compose version 1.29.2, build 5becea4c


# Создание образа docker:
$ docker-compose build

# Запуск приложения в docker-compose:
$ docker-compose up -d

# Просмотр логов в docker-compose:
$ docker-compose logs
$ docker-compose logs -f
$ docker-compose logs celery_worker
$ docker-compose logs -f celery_worker


# Вход в консоль контейнера в docker-compose:
$ docker-compose exec <service-name> bash
$ docker-compose exec web bash
$ docker-compose exec web python

# Вход в консоль контейнера который в данный момент не запущен:
# флаг --rm указывает докуру удалить контейнер после выхода из bash
$ docker-compose run --rm web bash


# Проверка работы docker-compose после запуска. Запуск в двух терминалах:
# терминал 1
$ docker-compose exec web python
>>> from manage import run_demo_task
>>> run_demo_task()
# терминал 2
$ docker-compose logs celery_worker
# output:
# celery_worker_1  | [2022-08-19 12:10:49,346: INFO/MainProcess] Task project.users.tasks.divide[01ae1ae4-9b3a-4751-912d-6c8eed85d20e] received
# celery_worker_1  | [2022-08-19 12:10:54,355: INFO/ForkPoolWorker-8] Task project.users.tasks.divide[01ae1ae4-9b3a-4751-912d-6c8eed85d20e] succeeded in 5.007303799999136s: 0.5


# Просмотр результатов работы celery прямо в redis:
# $ docker-compose exec redis sh
# $ redis-cli
$ docker-compose exec redis redis-cli
>>> MGET celery-task-meta-01ae1ae4-9b3a-4751-912d-6c8eed85d20e
# output:
# 1) "{\"status\": \"SUCCESS\", \"result\": 0.5, \"traceback\": null, \"children\": [], \"date_done\": \"2022-08-19T12:10:54.353792\", \"task_id\": \"01ae1ae4-9b3a-4751-912d-6c8eed85d20e\"}"


# Просмотр результатов работы celery в flower:
# открыть в браузере http://localhost:5557/

# Открыть FastAPI:
# открыть в браузере http://localhost:8010/


# + Отладка Celery в docker с помощью rdb.set_trace()
# 1. Добавим rdb.set_trace() в нужное место
# @shared_task
#def divide(x, y):
#    from celery.contrib import rdb
#    rdb.set_trace()
#    # ...

# 2. Выполним вызов run_demo_task()

# 3. Проверим логи и увидим приглашение подключиться по telnet
$ docker-compose logs -f
# celery_worker_1  | Remote Debugger:6903: Waiting for client...

# 4. Подключимся к rdb по telnet
$ docker-compose exec celery_worker bash
>>> (container)$ telnet 127.0.0.1 6903
# Trying 127.0.0.1...
# Connected to 127.0.0.1.
# Escape character is '^]'.
# > /app/project/users/tasks.py(9)divide()
# -> import time
# (Pdb):
# (Pdb): args
# x = 1
# y = 2

# 5. Чтобы выйти из отладки и продолжить выполнение кода нужно нажать "c" (continue)
# 6. Профит
# - Отладка Celery в docker с помощью rdb.set_trace()


# + Problem 1: Blocking Web Process
$ docker-compose up -d --build
> http://localhost:8010/users/form/
> Нажимаем Submit и смотрим в консоли браузера отладочную информацию
# - Problem 1: Blocking Web Process


# + Problem 2: Webhook Handler
$ docker-compose up -d --build
$ curl -X POST http://localhost:8010/users/webhook_test_bad/ -d {'data':'ping'}
$ curl -X POST http://localhost:8010/users/webhook_test_good/ -d {'data':'ping'}
# - Problem 2: Webhook Handler


# + Web-sockets example
$ docker-compose up -d --build
$ docker-compose logs -f
> http://localhost:8010/users/form_ws/
> Нажимаем Submit смотрим в консоль браузера или в Network -> WS -> Messages
# - Web-sockets example


# + Socket.io example
$ docker-compose up -d --build
$ docker-compose logs -f
> http://localhost:8010/users/form_socketio/
> Нажимаем Submit смотрим в консоль браузера или в Network -> WS -> Messages
# - Socket.io example


# + Celery multiple queues example

$ docker-compose down -v
$ docker-compose up -d --build
$ docker-compose logs -f

$ docker-compose run --rm celery_worker celery -A main.celery worker  -l info -Q low_priority

# - Celery multiple queues example


# + Celery routing example
$ docker-compose up -d --build
$ docker-compose exec web bash
(container)$ python

>>> from main import app
>>> from project.users.tasks import divide
# enqueue task to the default queue
>>> divide.delay(1, 2)

>>> from project.users.tasks import dynamic_example_one, dynamic_example_two, dynamic_example_three
# enqueue task to the default queue
>>> dynamic_example_one.delay()
# enqueue task to the high_priority queue
>>> dynamic_example_three.delay()
# enqueue task to the low_priority queue
>>> dynamic_example_two.delay()
# - Celery routing example


# + database transactions example
$ docker-compose logs -f
>>> http://localhost:8010/users/transaction_celery/
# - database transactions example


# + Logging example
$ docker-compose up -d --build
$ docker-compose exec web bash
(container)$ python

>>> from main import app
>>> from project.users.tasks import task_test_logger
>>> task_test_logger.delay()

# Теперь вы можете увидеть логи сelery в файле celery_tasks.log.log
# - Logging example

# + pytest example

$ docker-compose up -d --build
$ docker-compose exec web pytest
$ docker-compose exec web pytest --cov=.

# - pytest example


# + tdd example
# Apply the migrations:
$ docker-compose exec web bash
(container)$ alembic revision --autogenerate
(container)$ alembic upgrade head


docker-compose exec web pytest tests/tdd
docker-compose exec web pytest tests/tdd --cov=./project/tdd
# - tdd example


# + deployment

# 1. test

$ docker-compose stop
$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod up -d --build
$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod logs -f

# Gunicorn/FastAPI: http://localhost/docs
# Flower: http://localhost:5559 (use admin for the username and password)
# RabbitMQ dashboard: http://localhost:15672 (use admin for the username and password)

# 2. remove

$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod stop
$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod down -v
$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod ps

# 3. setup DigitalOcean

#(local)$ export DIGITAL_OCEAN_ACCESS_TOKEN=[your_digital_ocean_token]
#
## create Droplet
# (local)$ curl -X POST "https://api.digitalocean.com/v2/droplets" \
#     -H "Authorization: Bearer $DIGITAL_OCEAN_ACCESS_TOKEN" \
#     -d '{"name":"fastapi-celery-app","region":"sfo3","size":"s-2vcpu-4gb","image":"docker-18-04"}' \
#     -H "Content-Type: application/json"
#
## check status
# (local)$ curl \
#     -H 'Content-Type: application/json' \
#     -H 'Authorization: Bearer '$DIGITAL_OCEAN_ACCESS_TOKEN'' \
#     "https://api.digitalocean.com/v2/droplets?fastapi-celery-app"

# 4. generate an SSH key

(local)$ ssh root@<YOUR_INSTANCE_IP>

(server)$ ssh-keygen -t rsa
(server)$ cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys
(server)$ cat ~/.ssh/id_rsa

(local)$ export PRIVATE_KEY='-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEAqy8065H+/bn6e0NbPdoKgl7BI8bCLwJ2W1goI6UfVKN/w40P
yVEu0QDJgvZuzLqBEvZkeookpvYotQ4TddfY2ksVf3svDXsd6NZClJ/e8LawwVoP
VXL9Pdbo8X7PtCmvdD/lvuhcg8iFhwJR8YqxeZhRvds5PzwIhYx9/n7f3y6goR0s
8J71z47xZs6phQD96o3dG692E8gUBbt525p08+ysOQBLbv8DTdv0xoCOkV83I2z1
...
-----END RSA PRIVATE KEY-----'
(local)$ ssh-add - <<< "${PRIVATE_KEY}"
(local)$ ssh -o StrictHostKeyChecking=no root@<YOUR_INSTANCE_IP> whoami

# 5. final checking

(local)$ export DIGITAL_OCEAN_IP_ADDRESS=<YOUR_INSTANCE_IP>
(local)$ bash compose/auto_deploy_do.sh
# Uploading project...
# Uploaded complete.
# Building image...
# ...
# fastapi-celery-app: stopped  # after supervisard setup
# fastapi-celery-app: started  # after supervisard setup
# Build complete.

(server)$ cd /app
(server)$ docker-compose -f docker-compose.prod.yml up

# 6. setup Supervisor

(server)$ apt-get update
(server)$ apt-get install -y supervisor
(server)$ nano /etc/supervisor/conf.d/fastapi-celery-app.conf
# [program:fastapi-celery-app]
# directory=/app
# command=docker-compose -f docker-compose.prod.yml up
# autostart=true
# autorestart=true

(server)$ supervisorctl
# supervisor> reload
# Really restart the remote supervisord process y/N? y
# Restarted supervisord
#
# supervisor> status
# fastapi-celery-app                       STARTING
#
# supervisor> status
# fastapi-celery-app                       RUNNING   pid 10955, uptime 0:00:04

# - deployment


# + monitoring
$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod up -d --build

$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod exec web bash
# обзор текущих настроек и статистики celery в ком. строке:
(container)$ celery -A project.asgi.celery inspect stats
# обзор текущих и запланированных задач в ком. строке:
(container)$ celery -A project.asgi.celery inspect active
(container)$ celery -A project.asgi.celery inspect scheduled

# проверка очередей напрямую в redis:
$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod exec redis sh
(container)$ redis-cli llen default
# (integer) 19

# проверка очередей в RabbitMQ
$ docker-compose -f docker-compose.prod.yml -p fastapi-celery-prod exec rabbitmq sh
(container)$ rabbitmqctl list_queues name messages messages_ready messages_unacknowledged

# Timeout: 60.0 seconds ...
# Listing queues for vhost / ...
# name    messages        messages_ready  messages_unacknowledged
# celery@adafdf630408.celery.pidbox       0       0       0
# high_priority   0       0       0
# default 984     968     16
# celeryev.f9e18141-15f4-4fea-906b-0b46a45d1c7e   0       0       0
# low_priority    0       0       0
# celeryev.726dd699-d7d8-46fa-868d-952419a5935c   0       0       0
# - monitoring

# + cAdvisor

# docker run --volume=/:/rootfs:ro --volume=/var/run:/var/run:rw --volume=/sys:/sys:ro --volume=/var/lib/docker/:/var/lib/docker:ro --publish=8080:8080 --detach=true --name=cadvisor google/cadvisor

$ docker run \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:rw \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  --name=cadvisor \
  google/cadvisor

# If you have trouble with the above command on Mac,
# try to add --volume=/var/run/docker.sock:/var/run/docker.sock:ro
# and check this Github issue: https://github.com/google/cadvisor/issues/1565

prometheus:
http://localhost:9090/graph
query: container_memory_usage_bytes{name="fastapi-celery-prod_celery_worker_1"}
# - cAdvisor


# + best practics

# 1. SSL is more secure so it's highly recommended.

CELERY_BROKER_USE_SSL = True
CELERY_REDIS_BACKEND_USE_SSL = True

REDIS_URL = "rediss://{username}:{password}@{host}:{port}?ssl_cert_reqs=CERT_NONE"
CELERY_BROKER_URL = REDIS_URL

# 2. As mentioned, it's recommended to change the default queue name from celery to default
#    and to set CELERY_TASK_CREATE_MISSING_QUEUES to False to prevent typos in your code. For example:

# CELERY_TASK_DEFAULT_QUEUE: str = "default"

# 3. Celery's method for prefetching is not very efficient, both dynamically and globally. It can actually cause problems quite often.
#    We recommend limiting prefetching to one so that each worker gets only one message at a time:

# CELERY_WORKER_PREFETCH_MULTIPLIER = 1

# 4. Celery workers send an acknowledgement back to the message broker after a task is picked up from the queue.
#    The broker will usually respond by removing the task from the queue.
#    This can cause problems if the worker dies while running the task and the task has been removed from the queue.

# CELERY_TASK_ACKS_LATE = True

# 5. Time limit
# a)
# CELERY_TASK_SOFT_TIME_LIMIT = 15 * 60
# CELERY_TASK_TIME_LIMIT = CELERY_TASK_SOFT_TIME_LIMIT + 30

# b)
# # per task when defined
# @celery.task(time_limit=30, soft_time_limit=10)
# def your_task():
#     try:
#         return do_something()
#     except SoftTimeLimitExceeded:
#         cleanup_in_a_hurry()

# c)
# # per task when called
# your_task.apply_async(args=[], kwargs={}, time_limit=30, soft_time_limit=10)

# If the task exceeds the soft limit, SoftTimeLimitExceeded will be raised.

# Serializer

# CELERY_TASK_SERIALIZER = json

# https://docs.celeryq.dev/en/stable/history/whatsnew-4.0.html?highlight=ofair#ofair-is-now-the-default-scheduling-strategy
# https://docs.celeryq.dev/en/stable/userguide/tasks.html#state
# https://docs.celeryq.dev/en/stable/userguide/canvas.html
# https://docs.celeryq.dev/en/stable/userguide/optimizing.html
# https://devchecklists.com/celery-tasks-checklist/

# - best practics
